{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ab7a7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Modeling: Fraud Detection\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedKFold, GridSearchCV, cross_validate\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Modeling: Fraud Detection\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer, average_precision_score, confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# -------------------------------\n",
    "# Load processed data\n",
    "# -------------------------------\n",
    "fraud = pd.read_csv(\"../data/processed/fraud_processed.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# Features and target\n",
    "# -------------------------------\n",
    "y = fraud['class']\n",
    "X = fraud.drop(columns=['class','signup_time','purchase_time','ip_address','device_id','user_id'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Train-test split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Scaling\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Handle class imbalance with SMOTE\n",
    "# -------------------------------\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# Logistic Regression (Baseline)\n",
    "# -------------------------------\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "lr.fit(X_train_res, y_train_res)\n",
    "\n",
    "lr_preds = lr.predict(X_test_scaled)\n",
    "lr_probs = lr.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(\"F1:\", f1_score(y_test, lr_preds))\n",
    "print(\"AUC-PR:\", average_precision_score(y_test, lr_probs))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, lr_preds))\n",
    "\n",
    "# -------------------------------\n",
    "# Random Forest with Hyperparameter Tuning\n",
    "# -------------------------------\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "scorer = make_scorer(f1_score)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring=scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train_res, y_train_res)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "print(\"\\nBest RF Hyperparameters:\", grid_rf.best_params_)\n",
    "\n",
    "# -------------------------------\n",
    "# XGBoost with Hyperparameter Tuning\n",
    "# -------------------------------\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=(y_train==0).sum() / (y_train==1).sum(),\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring=scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train_res, y_train_res)\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "\n",
    "print(\"\\nBest XGBoost Hyperparameters:\", grid_xgb.best_params_)\n",
    "\n",
    "# -------------------------------\n",
    "# Cross-Validation Metrics\n",
    "# -------------------------------\n",
    "models = {\n",
    "    \"Logistic Regression\": lr,\n",
    "    \"Random Forest\": best_rf,\n",
    "    \"XGBoost\": best_xgb\n",
    "}\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X_train_res,\n",
    "        y_train_res,\n",
    "        cv=cv,\n",
    "        scoring={'F1': make_scorer(f1_score), 'AUC-PR': 'average_precision'},\n",
    "        return_train_score=False\n",
    "    )\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'F1_mean': np.mean(scores['test_F1']),\n",
    "        'F1_std': np.std(scores['test_F1']),\n",
    "        'AUC-PR_mean': np.mean(scores['test_AUC-PR']),\n",
    "        'AUC-PR_std': np.std(scores['test_AUC-PR'])\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "print(\"\\nCross-Validation Results:\\n\", cv_df)\n",
    "\n",
    "# -------------------------------\n",
    "# Model Evaluation on Test Set\n",
    "# -------------------------------\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    probs = model.predict_proba(X_test_scaled)[:,1]\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    auc_pr = average_precision_score(y_test, probs)\n",
    "    print(f\"\\n{name} Test Performance:\")\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"AUC-PR:\", auc_pr)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
    "\n",
    "# -------------------------------\n",
    "# Model Selection Justification\n",
    "# -------------------------------\n",
    "\"\"\"\n",
    "Based on cross-validation metrics and test performance:\n",
    "- Logistic Regression: interpretable but lower F1 and AUC-PR\n",
    "- Random Forest: higher performance, medium interpretability\n",
    "- XGBoost: best F1 and AUC-PR, slightly less interpretable but manageable with SHAP\n",
    "\n",
    "=> Selected Model: XGBoost as the best balance of predictive power and interpretability.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
